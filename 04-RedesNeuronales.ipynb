{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales Artificiales\n",
    "## Perceptrón\n",
    "\n",
    "El preceptrón es una estructura que trata de imitar el funcionamiento de una neurona.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Blausen_0657_MultipolarNeuron.png/1024px-Blausen_0657_MultipolarNeuron.png\" alt=\"Neurona\" style=\"width: 400px;\"/>\n",
    "\n",
    "> Fig. 1: [Imágen Neurona Wikipedia](https://en.wikipedia.org/wiki/Neuron) <br>\n",
    "\n",
    "Donde el modelo se simplifica a: <br>\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/31/Perceptron.svg\" alt=\"Perceptrón\" style=\"width: 400px;\"/>\n",
    "\n",
    "> Fig. 2: [Imágen Perceptrón Wikipedia](https://en.wikipedia.org/wiki/Perceptron) <br>\n",
    "\n",
    "Considerando $\\overline{x}=(x_{1}, x_{2},...,x_{n})$ y $\\overline{w}=(w_{1}, w_{2},...,w_{n})$: <br>\n",
    "$$percept(\\overline{x})=f(\\overline{x} \\cdot \\overline{w} + b)=f(\\sum(x_{i} \\cdot x_{i} )+ b)$$\n",
    "Dependiendo como se seleccione $f(o)$, un perceptrón es simirar a una regresión lineal ($f(o)=o$) o a una regresión lógistica $f(o)=\\frac{1}{1+e^{-o}}$. Obviamente, existen otras funciones de activación que se irán discutiendo a lo largo del curso.\n",
    "\n",
    "Combinado diversos preceptrones en forma paralela se forma una capa de una red neuronal. En este caso, la capa se conoce como una capa densa, ya que todas las salidas de la capa están conectados con cada entrada. En el caso de una capa, $W$ es una matriz de dimensiones $cantidad\\ de\\ características$ X $cantidad\\ de\\ perceptrones$, $b$ es un vector de dimensionalidad $cantidad\\ de\\ perceptrones$, y $f(o)$ se aplíca elemento a elemento del resultado. Considerando el trabajo práctico de la clase anterior $W$ es una matriz de 786 X 10, y $b$ es un vector de 10 elementos.\n",
    "\n",
    "## Keras\n",
    "[Keras](https://keras.io/) en una librería de alto nivel para redes neuronales que abstrae las operaciones más comunes de las redes neuronales facilitando la escritura de un código más limpio. Keras soporta diversos backend para realizar las operaciones aritméticas, por ejemplo [TensorFlow](https://keras.io/backend/), [Theano](http://deeplearning.net/software/theano/) y [CNTK](https://www.microsoft.com/en-us/cognitive-toolkit/).\n",
    "A continuación se muesta un ejemplo de utilización de Keras para el problema clasificación de MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model \n",
    "from keras.utils import to_categorical\n",
    "\n",
    "print('Cargando el dataset')\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "size = x_train.shape[1]*x_train.shape[2]\n",
    "x_train = x_train.reshape((x_train.shape[0], size)) / 255\n",
    "x_test = x_test.reshape((x_test.shape[0], size)) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicializo la entrada como un vector de tamaño size\n",
    "i = Input(shape=(size,)) \n",
    "#Inicializo una capa densa, activación sigmoide y entrada i\n",
    "#Para inicializar la capa densa se usa la API funcional de keras\n",
    "d = Dense(10, activation='sigmoid')(i) \n",
    "#Inicializo el modelo a partir de sus entradas y salidas\n",
    "model = Model(inputs=i, outputs=d)\n",
    "#Compilo el modelo con la función de pedidad y utilizando \n",
    "#Stocastic Gradiant Descent como optimizador (una variante del Gradient Descent)\n",
    "#metrics no es necesario, pero permite usar otra función de error para la validación\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['categorical_accuracy'])\n",
    "#Muestro la esturctura del perceptrón\n",
    "model.summary()\n",
    "#Entreno y guardo el historial del errores en h. \n",
    "#verbose: 0: sin salida, 1: salida detallada, 2: salida solo al final del epoch\n",
    "h = model.fit(x_train, to_categorical(y_train), \n",
    "              batch_size=1000, epochs=100, \n",
    "              validation_data=(x_test, to_categorical(y_test)), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy del clasificador en validación: {}'.format(\n",
    "        accuracy_score(y_test, \n",
    "                       np.argmax(model.predict(x_test), axis=1))))\n",
    "\n",
    "print('Valor del error por epoch')\n",
    "\n",
    "v_loss = h.history['val_loss']\n",
    "loss = h.history['loss']\n",
    "plt.plot(range(len(loss)), loss, '-r')\n",
    "plt.plot(range(len(v_loss)), v_loss, '-b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.show()\n",
    "\n",
    "print('Valor del accuracy por epoch')\n",
    "\n",
    "plt.plot(range(len(v_loss)), h.history['categorical_accuracy'], '-b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stocastic Gradient Descent\n",
    "Hasta ahora estabamos usando _gradient descent_ en el cual se calculaba el gradiente de la función de error sobre todas las instancias de entrenamiento. Esto puede ser muy costoso o impracticable si el conjunto de datos de entrenamiento es muy grande. Para solucionar este problema se usa el _Stocastic Gradient Descent_, que intenta aproximar el gradiente de forma aleatoria.\n",
    "\n",
    "```\n",
    "w = parámetros_a_entrenar()\n",
    "x, y = dataset_entrenaminto()\n",
    "for e in range(epochs):\n",
    "    aleatorizar(x, y)\n",
    "    for x_a, y_a in separar_en_batchs(x, y, tamaño_batch):\n",
    "        g = calcular_gradiente(perdidad(x, y, w), w)\n",
    "        w = w - lr * g\n",
    "\n",
    "```\n",
    "Además, como la actualización se hace más seguido, es posible usar un `lr` menor mejorando la búsqueda. Por defecto, Keras inicializa el `lr=0.01` para el _Stocastic Gradient Descent_. Es importante notar que si el tamaño del batch es igual a la cantidad de elementos en el conjunto de entrenamiento el _Stocastic Gradient Descent_ es efectivamente un _Gradient Descent_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importa clases necesarias para la prueba.\n",
    "#Si Google Colab falla al importar agregar la linea \"!pip install tqdm\" sin las comillas\n",
    "#para instalar el paquete. Es una barra de progreso\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from keras.optimizers import SGD\n",
    "#Inicializa una regresión lógistica\n",
    "i = Input(shape=(size,)) \n",
    "d = Dense(10, activation='sigmoid')(i) \n",
    "model = Model(inputs=i, outputs=d)\n",
    "#Uso SGD con un lr de 0.1 para mostrar mejor los efectos.\n",
    "model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.1), metrics=['categorical_accuracy'])\n",
    "#Paso y_train e y_test al formato categórico\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "\n",
    "#Configuración de la prueba\n",
    "batch_size = 10\n",
    "cant = 100\n",
    "sample = 1\n",
    "\n",
    "error_train_batch = []\n",
    "error_test_batch = []\n",
    "\n",
    "#Esto se haría dentro de un epoch\n",
    "xt, yt = shuffle(x_train, y_train_cat)\n",
    "indexes = range(0, xt.shape[0], batch_size)\n",
    "\n",
    "indexes = indexes[:min(cant, len(indexes))] # Esto es solo para limitar la cantidad de pruebas\n",
    "for i, j in tqdm(list(enumerate(indexes))):\n",
    "    model.fit(xt[j:min(j + batch_size, yt.shape[0]), :], \n",
    "              yt[j:min(j + batch_size, yt.shape[0]), :], \n",
    "              batch_size=batch_size, epochs=1, \n",
    "              verbose=0)\n",
    "    #Cada \"sample\" batches tomo una medición de los errores\n",
    "    if i % sample == 0:\n",
    "        error_train_batch.append(model.evaluate(x_train, y_train_cat, verbose=0)[0])\n",
    "        error_test_batch.append(model.evaluate(x_test, y_test_cat, verbose=0)[0])\n",
    "#Imprimo el error por batch\n",
    "print('Valor del error por batch')\n",
    "\n",
    "plt.plot(range(0, sample * len(error_train_batch), sample), error_train_batch, '-r')\n",
    "plt.plot(range(0, sample * len(error_train_batch), sample), error_test_batch, '-b')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otras variaciones del SGD\n",
    "También se suele usar un factor de momento para hacer que esta curva de error sea más suavizada. La efectividad del uso de momento depende del dataset. A continuación se preseta el algoritmo modificado. `v` representa la velocidad de cambio, `m` es el parámetro que asigna importancia a la velocidad de cambio anterior (generalmente se utiliza 0.9).\n",
    "```\n",
    "w = parámetros_a_entrenar()\n",
    "v = ceros_como(w)\n",
    "x, y = dataset_entrenaminto()\n",
    "for e in range(epochs):\n",
    "    aleatorizar(x, y)\n",
    "    for x_a, y_a in separar_en_batchs(x, y, tamaño_batch):\n",
    "        g = calcular_gradiente(perdidad(x, y, w), w)\n",
    "        v = momento * v - lr * g\n",
    "        w = w + v\n",
    "\n",
    "```\n",
    "Existen más variaciones sobre la técnicas basadas en gradiente. Si bien se irán cubriendo este tema durante el curso, se recomienda la lectura de [An overview of gradient descent optimization algorithms](https://arxiv.org/abs/1609.04747). En particular, la sección 4. Más información sobre este y otros optimizadores se encuentra disponible en la documentación de [los optimizadores de Keras](https://keras.io/optimizers/)\n",
    "## Ejercicio\n",
    "Utiliza batch de 60000 para utilizar el _Stocastic Gradient Descent_ como _Gradient Descent_. Pruebe con 100 epochs por cuestiones de tiempo. \n",
    "\n",
    "1. ¿Qué sucede cuando se utiliza el `lr` por defecto?\n",
    "1. ¿Qué sucede cuando se incrementa el `lr` a 0.1?\n",
    "1. ¿Qué sucede cuando se incrementa el `lr` a 1?\n",
    "\n",
    "_Opcional_: Conteste las mismas preguntas con más epochs, por ejemplo 1000 o 10000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escriba su código aquí."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax y Categorical_crossentropy\n",
    "El problema de NMIST es un problema de clasificación múlticlases o categórico, donde una imagen representa un y solo un dígito. Sin embargo, cuando entrenamos la regresión logística no hacemos uso de ese hecho, tanto las función de activación sigmoide como la función de error _binary crossentropy_ (equivalente a la entropía cruzada que definimos en el práctico anterior) trabajan cada elemento de las predicciones de forma independiente.\n",
    "## Softmax\n",
    "Softmax es una función de activación que transforma la salida de la regresión a una distribución de probabilidades. Es decir, la suma de cada vector de predicciones es 1 y el valor de cada elemento es la probabilidad de que la instancia clasificada pertenezca a esa clase.\n",
    "\n",
    "$$softmax(o)_j=\\frac{e^{o_j}}{\\sum e^{o_i}}$$\n",
    "\n",
    "Esta función se aplica a cada elemento, pero conoce los otros elementos del vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = Input(shape=(size,)) \n",
    "d = Dense(10, activation='softmax')(i) \n",
    "model = Model(inputs=i, outputs=d)\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['categorical_accuracy'])\n",
    "\n",
    "print('Entrenando con softmax')\n",
    "h = model.fit(x_train, to_categorical(y_train), \n",
    "              batch_size=1000, epochs=100, \n",
    "              validation_data=(x_test, to_categorical(y_test)), verbose=0)\n",
    "print('Accuracy del clasificador en validación: {}'.format(\n",
    "        accuracy_score(y_test, \n",
    "                       np.argmax(model.predict(x_test), axis=1))))\n",
    "\n",
    "print('Valor del error por epoch')\n",
    "\n",
    "v_loss = h.history['val_loss']\n",
    "loss = h.history['loss']\n",
    "plt.plot(range(len(loss)), loss, '-r')\n",
    "plt.plot(range(len(v_loss)), v_loss, '-b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.show()\n",
    "\n",
    "print('Valor del accuracy por epoch')\n",
    "\n",
    "plt.plot(range(len(v_loss)), h.history['categorical_accuracy'], '-b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Crossentropy\n",
    "Esta función de error considera el error sobre la categoría real, normalizando el valor de la predicción. Considerando $\\hat{y}=(\\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_C)$ in vector de valores asociados a las clases\n",
    "$$P_\\hat{y}=\\frac{\\hat{y}}{\\sum\\hat{y}_i}$$\n",
    "$$CCE(y,P_\\hat{y})=-\\frac{\\sum y * log(P_\\hat{y})}{N} $$\n",
    "Notese que el valor de error se considera solo sobre las clases verdaderas, las otras son afectadas a través de la normalización de la salida $\\hat{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = Input(shape=(size,)) \n",
    "d = Dense(10, activation='sigmoid')(i) \n",
    "model = Model(inputs=i, outputs=d)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['categorical_accuracy'])\n",
    "\n",
    "print('Entrenando con softmax')\n",
    "h = model.fit(x_train, to_categorical(y_train), \n",
    "              batch_size=1000, epochs=100, \n",
    "              validation_data=(x_test, to_categorical(y_test)), verbose=0)\n",
    "print('Accuracy del clasificador en validación: {}'.format(\n",
    "        accuracy_score(y_test, \n",
    "                       np.argmax(model.predict(x_test), axis=1))))\n",
    "\n",
    "print('Valor del error por epoch')\n",
    "\n",
    "v_loss = h.history['val_loss']\n",
    "loss = h.history['loss']\n",
    "plt.plot(range(len(loss)), loss, '-r')\n",
    "plt.plot(range(len(v_loss)), v_loss, '-b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.show()\n",
    "\n",
    "print('Valor del accuracy por epoch')\n",
    "\n",
    "plt.plot(range(len(v_loss)), h.history['categorical_accuracy'], '-b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escriba código para entrenar una salida con softmax y categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal: Perceptrón multi-capa\n",
    "La red neuronal más sencilla, conocida como perceptrón multi-capa, no es más que capas de preceptrones aplicadas una sobre la otra.\n",
    "\n",
    "Entonces un preceptron multi-capa tiene la siguiente forma:\n",
    "\n",
    "$$l_{1}=f_{1}(\\overline{x} \\cdot W_{1} + \\overline{bias_{1}})$$\n",
    "\n",
    "$$l_{2}=f_{2}(\\overline{l_{1}} \\cdot W_{2} + \\overline{bias_{2}})$$\n",
    "\n",
    "$$...$$\n",
    "\n",
    "$$l_{N}=f_{N}(\\overline{l_{N-1}} \\cdot W_{N} + \\overline{bias_{N}})$$\n",
    "\n",
    "Es importante destacar que, dado una función de error, calcular el gradiente para cada parámetro de la red, sea $W_{i}$ o $bias_{i}$, es simplemente aplicar la regla de la cadena en repetidas oscaciones.\n",
    "\n",
    "Uno de los motivos para la existencia de los preceptrones multicapas, es que no todos los problemas se pueden resolver con funciones aplicadas sobre operaciones lineales. Por ejemplo, no se puede hacer una regresión logística (ni lineal) que aproxime la función XOR. Considerando el dataset:\n",
    "\n",
    "| $X_0$ | $X_1$ | $Y$ |\n",
    "| --- | --- | --- |\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.asarray([[0], [1], [1], [0]])\n",
    "i = Input(shape=(2,)) \n",
    "d = Dense(1, activation='sigmoid')(i) \n",
    "model = Model(inputs=i, outputs=d)\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['binary_accuracy'])\n",
    "h = model.fit(x, y, batch_size=1, epochs=10000, verbose=0)\n",
    "print(model.predict(x))\n",
    "\n",
    "print('Valor del error por epoch')\n",
    "loss = h.history['loss']\n",
    "plt.plot(range(len(loss)), loss, '-r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero al agregar una nueva capa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = Input(shape=(2,)) \n",
    "d = Dense(8, activation='sigmoid')(i) \n",
    "d = Dense(1, activation='sigmoid')(d)\n",
    "model = Model(inputs=i, outputs=d)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['binary_accuracy'])\n",
    "h = model.fit(x, y, batch_size=1, epochs=10000, verbose=0)\n",
    "print(model.predict(x))\n",
    "\n",
    "print('Valor del error por epoch')\n",
    "loss = h.history['loss']\n",
    "plt.plot(range(len(loss)), loss, '-r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De hecho, se ha demostrados [1] que $F(X)=\\sum v_i * sigmoid(X \\dot W)$, es decir una red con una sola capa oculta, puede aproximar cualquier funcion $f(X)$. Sin embargo, la prueba no dice nada de cuantas neuronas ocultas se necesitan, ni de como entrenar los parámetros.\n",
    "\n",
    "[1] Cybenko, G. (1989)  \"[Approximations by superpositions of sigmoidal functions](https://doi.org/10.1007%2FBF02551274)\", Mathematics of Control, Signals, and Systems, 2(4), 303-314."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = Input(shape=(2,)) \n",
    "d = Dense(8, activation='tanh')(i) \n",
    "d = Dense(1, activation='sigmoid')(d)\n",
    "model = Model(inputs=i, outputs=d)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['binary_accuracy'])\n",
    "h = model.fit(x, y, batch_size=1, epochs=1000, verbose=0)\n",
    "print(model.predict(x))\n",
    "\n",
    "print('Valor del error por epoch')\n",
    "loss = h.history['loss']\n",
    "plt.plot(range(len(loss)), loss, '-r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajo práctico\n",
    "Implemente una red con más de una capa para resolver el problema de MNIST. Consideraciones:\n",
    "* Utilice la función de activación softmax para la última capa y categorical cross entropy como función de error.\n",
    "* Pruebe diversas cantidad de capas ocultas (entre 1 y 4).\n",
    "* Pruebe distintas funciones de activación para la capa intermedia. Por ejemplo: sigmoide, tangente hiperbólica, relu, o lineal. Puede encontrar más información sobre las funciones de activación en [Keras](https://keras.io/activations/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escríba su código aquí."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
